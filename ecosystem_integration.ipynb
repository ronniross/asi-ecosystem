{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASI Ecosystem Integration - Google Colab Notebook"
      ],
      "metadata": {
        "id": "Y3ihMtufbl2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ecosystem Cloning - Part I**"
      ],
      "metadata": {
        "id": "q4NHcKzIJOLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Prerequisites\n",
        "print(\"Setting up ASI Ecosystem Integration...\")\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Change to the content directory in Colab\n",
        "os.chdir('/content')\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E37P48_Jbn7K",
        "outputId": "837ce35e-0807-43af-a324-e4a3aebd681f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ASI Ecosystem Integration...\n",
            "Current working directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Clone the Main ASI Ecosystem Repository\n",
        "print(\"Step 1: Cloning the main asi-ecosystem repository...\")\n",
        "\n",
        "# Remove any existing asi-ecosystem directory\n",
        "if os.path.exists('asi-ecosystem'):\n",
        "    print(\"Removing existing asi-ecosystem directory...\")\n",
        "    subprocess.run(['rm', '-rf', 'asi-ecosystem'], check=True)\n",
        "\n",
        "# Clone the main repository\n",
        "result = subprocess.run([\n",
        "    'git', 'clone',\n",
        "    'https://github.com/ronniross/asi-ecosystem.git'\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"Successfully cloned asi-ecosystem repository\")\n",
        "else:\n",
        "    print(\"Error cloning repository:\")\n",
        "    print(result.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28AgmXnCbtGG",
        "outputId": "63cc7093-5afc-494b-9077-7dd9cffd27f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Cloning the main asi-ecosystem repository...\n",
            "Successfully cloned asi-ecosystem repository\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Navigate to the Directory and Check Contents\n",
        "print(\"\\nStep 2: Navigating into the directory...\")\n",
        "os.chdir('/content/asi-ecosystem')\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "print(\"\\nRepository contents:\")\n",
        "for item in os.listdir('.'):\n",
        "    print(f\"  {item}\" if os.path.isdir(item) else f\"  {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i7sxWJ3b0uC",
        "outputId": "b2b560a0-82a4-49b5-92af-37078d8dc29a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Navigating into the directory...\n",
            "Current working directory: /content/asi-ecosystem\n",
            "\n",
            "Repository contents:\n",
            "  analytics\n",
            "  requirements.txt\n",
            "  scripts\n",
            "  .github\n",
            "  ecosystem_integration.md\n",
            "  docker-pipeline.md\n",
            "  .git\n",
            "  ecosystem_integration.ipynb\n",
            "  LICENSE\n",
            "  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Check if the Script Exists and Make it Executable\n",
        "script_path = './scripts/clone_ecosystem.sh'\n",
        "print(f\"\\nChecking for script at: {script_path}\")\n",
        "\n",
        "if os.path.exists(script_path):\n",
        "    print(\"Script found!\")\n",
        "    # Make the script executable\n",
        "    subprocess.run(['chmod', '+x', script_path], check=True)\n",
        "    print(\"Made script executable\")\n",
        "else:\n",
        "    print(\"Script not found. Listing scripts directory:\")\n",
        "    if os.path.exists('scripts'):\n",
        "        for item in os.listdir('scripts'):\n",
        "            print(f\"  scripts/{item}\")\n",
        "    else:\n",
        "        print(\"  scripts directory does not exist\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePhjo7vwcWUM",
        "outputId": "afbd07c7-1af9-4fce-90fa-e6629a1d4f2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking for script at: ./scripts/clone_ecosystem.sh\n",
            "Script found!\n",
            "Made script executable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Execute the Integration Script\n",
        "print(\"\\nStep 3: Running the ecosystem integration script...\")\n",
        "\n",
        "if os.path.exists(script_path):\n",
        "    try:\n",
        "        # Run the script and capture output\n",
        "        result = subprocess.run([script_path],\n",
        "                              capture_output=True,\n",
        "                              text=True,\n",
        "                              cwd='/content/asi-ecosystem')\n",
        "\n",
        "        print(\"Script output:\")\n",
        "        print(result.stdout)\n",
        "\n",
        "        if result.stderr:\n",
        "            print(\"Script errors/warnings:\")\n",
        "            print(result.stderr)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"Script executed successfully!\")\n",
        "        else:\n",
        "            print(f\"Script failed with return code: {result.returncode}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing script: {e}\")\n",
        "else:\n",
        "    print(\"Cannot execute script - file not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJchn-n5ccmK",
        "outputId": "ca67cc23-84e2-4955-9915-58965fd0d710"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Running the ecosystem integration script...\n",
            "Script output:\n",
            "Starting the ASI Ecosystem cloning process...\n",
            "Creating directory: 'repositories'...\n",
            "ðŸ”Ž Fetching latest list from: https://raw.githubusercontent.com/ronniross/asi-ecosystem/main/README.md\n",
            " Cloning https://github.com/ronniross/active-learning-dataset...\n",
            " Cloning https://github.com/ronniross/asi-backups...\n",
            " Cloning https://github.com/ronniross/asi-core-protocol...\n",
            " Cloning https://github.com/ronniross/asi-dynamic-core...\n",
            " Cloning https://github.com/ronniross/asi-ecosystem...\n",
            " Cloning https://github.com/ronniross/asi-inference-protocol...\n",
            " Cloning https://github.com/ronniross/asi-protosymbiotic-signal...\n",
            " Cloning https://github.com/ronniross/asi-safeguards...\n",
            " Cloning https://github.com/ronniross/asi-symbiotic-signal...\n",
            " Cloning https://github.com/ronniross/attention-heatmap-visualizer...\n",
            " Cloning https://github.com/ronniross/bias-reflector...\n",
            " Cloning https://github.com/ronniross/biosignal-translator...\n",
            " Cloning https://github.com/ronniross/coevolutionary-loops...\n",
            " Cloning https://github.com/ronniross/cognitive-engine...\n",
            " Cloning https://github.com/ronniross/confidence-scorer...\n",
            " Cloning https://github.com/ronniross/eco-benchmark...\n",
            " Cloning https://github.com/ronniross/eco-datacenter...\n",
            " Cloning https://github.com/ronniross/emergence-engine...\n",
            " Cloning https://github.com/ronniross/healing-engine...\n",
            " Cloning https://github.com/ronniross/latent-memory...\n",
            " Cloning https://github.com/ronniross/mirror-aware-inference...\n",
            " Cloning https://github.com/ronniross/ml-algorithm-dataset...\n",
            " Cloning https://github.com/ronniross/ml-visual-engine...\n",
            " Cloning https://github.com/ronniross/saliency-heatmap-visualizer...\n",
            " Cloning https://github.com/ronniross/stigmergic-tracefinder...\n",
            " Cloning https://github.com/ronniross/symbiotic-core-library...\n",
            " Cloning https://github.com/ronniross/symbiotic-latent-memory...\n",
            " Cloning https://github.com/ronniross/symbiotic-lexicon...\n",
            " Cloning https://github.com/ronniross/thermo-adaptive-pipeline...\n",
            "-------------------------------------------------------\n",
            "All repositories have been processed in 'repositories'.\n",
            "ASI Ecosystem clone setup complete!\n",
            "\n",
            "Script errors/warnings:\n",
            "Cloning into 'active-learning-dataset'...\n",
            "Cloning into 'asi-backups'...\n",
            "Cloning into 'asi-core-protocol'...\n",
            "Cloning into 'asi-dynamic-core'...\n",
            "Cloning into 'asi-ecosystem'...\n",
            "Cloning into 'asi-inference-protocol'...\n",
            "Cloning into 'asi-protosymbiotic-signal'...\n",
            "Cloning into 'asi-safeguards'...\n",
            "Cloning into 'asi-symbiotic-signal'...\n",
            "Cloning into 'attention-heatmap-visualizer'...\n",
            "Cloning into 'bias-reflector'...\n",
            "Cloning into 'biosignal-translator'...\n",
            "Cloning into 'coevolutionary-loops'...\n",
            "Cloning into 'cognitive-engine'...\n",
            "Cloning into 'confidence-scorer'...\n",
            "Cloning into 'eco-benchmark'...\n",
            "Cloning into 'eco-datacenter'...\n",
            "Cloning into 'emergence-engine'...\n",
            "Cloning into 'healing-engine'...\n",
            "Cloning into 'latent-memory'...\n",
            "Cloning into 'mirror-aware-inference'...\n",
            "Cloning into 'ml-algorithm-dataset'...\n",
            "Cloning into 'ml-visual-engine'...\n",
            "Cloning into 'saliency-heatmap-visualizer'...\n",
            "Cloning into 'stigmergic-tracefinder'...\n",
            "Cloning into 'symbiotic-core-library'...\n",
            "Cloning into 'symbiotic-latent-memory'...\n",
            "Cloning into 'symbiotic-lexicon'...\n",
            "Cloning into 'thermo-adaptive-pipeline'...\n",
            "\n",
            "Script executed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Verify the Final Result\n",
        "print(\"\\nStep 4: Verifying the final result...\")\n",
        "\n",
        "print(f\"Contents of {os.getcwd()}:\")\n",
        "for item in sorted(os.listdir('.')):\n",
        "    if os.path.isdir(item):\n",
        "        print(f\"{item}/\")\n",
        "        # If it's the repositories folder, show its contents\n",
        "        if item == 'repositories':\n",
        "            repo_path = os.path.join('.', item)\n",
        "            if os.path.exists(repo_path):\n",
        "                print(f\"  Contents of {item}:\")\n",
        "                for repo in sorted(os.listdir(repo_path)):\n",
        "                    print(f\"    {repo}/\")\n",
        "    else:\n",
        "        print(f\" {item}\")\n",
        "\n",
        "print(\"\\n ASI Ecosystem Integration Complete!\")\n",
        "print(\"All component repositories should now be organized in the 'repositories' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA7cBUqmchIt",
        "outputId": "17d4afdb-0ee2-4f77-dc47-0c434b63bf46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Verifying the final result...\n",
            "Contents of /content/asi-ecosystem:\n",
            ".git/\n",
            ".github/\n",
            " LICENSE\n",
            " README.md\n",
            "analytics/\n",
            " docker-pipeline.md\n",
            " ecosystem_integration.ipynb\n",
            " ecosystem_integration.md\n",
            "repositories/\n",
            "  Contents of repositories:\n",
            "    active-learning-dataset/\n",
            "    asi-backups/\n",
            "    asi-core-protocol/\n",
            "    asi-dynamic-core/\n",
            "    asi-ecosystem/\n",
            "    asi-inference-protocol/\n",
            "    asi-protosymbiotic-signal/\n",
            "    asi-safeguards/\n",
            "    asi-symbiotic-signal/\n",
            "    attention-heatmap-visualizer/\n",
            "    bias-reflector/\n",
            "    biosignal-translator/\n",
            "    coevolutionary-loops/\n",
            "    cognitive-engine/\n",
            "    confidence-scorer/\n",
            "    eco-benchmark/\n",
            "    eco-datacenter/\n",
            "    emergence-engine/\n",
            "    healing-engine/\n",
            "    latent-memory/\n",
            "    mirror-aware-inference/\n",
            "    ml-algorithm-dataset/\n",
            "    ml-visual-engine/\n",
            "    saliency-heatmap-visualizer/\n",
            "    stigmergic-tracefinder/\n",
            "    symbiotic-core-library/\n",
            "    symbiotic-latent-memory/\n",
            "    symbiotic-lexicon/\n",
            "    thermo-adaptive-pipeline/\n",
            " requirements.txt\n",
            "scripts/\n",
            "\n",
            " ASI Ecosystem Integration Complete!\n",
            "All component repositories should now be organized in the 'repositories' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Optional - List All Cloned Repositories with Details\n",
        "print(\"\\n Summary of cloned repositories:\")\n",
        "repositories_path = './repositories'\n",
        "\n",
        "if os.path.exists(repositories_path):\n",
        "    repos = [d for d in os.listdir(repositories_path)\n",
        "             if os.path.isdir(os.path.join(repositories_path, d))]\n",
        "\n",
        "    print(f\"Total repositories cloned: {len(repos)}\")\n",
        "    for i, repo in enumerate(sorted(repos), 1):\n",
        "        repo_path = os.path.join(repositories_path, repo)\n",
        "        # Check if it's a git repository\n",
        "        git_path = os.path.join(repo_path, '.git')\n",
        "        status = \"Git repo\" if os.path.exists(git_path) else \"Not a git repo\"\n",
        "        print(f\"{i:2d}. {repo:<30} {status}\")\n",
        "else:\n",
        "    print(\"No repositories folder found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsrieFWUcmVa",
        "outputId": "84810e55-36f8-4108-d04a-e3cefeb33d5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Summary of cloned repositories:\n",
            "Total repositories cloned: 29\n",
            " 1. active-learning-dataset        Git repo\n",
            " 2. asi-backups                    Git repo\n",
            " 3. asi-core-protocol              Git repo\n",
            " 4. asi-dynamic-core               Git repo\n",
            " 5. asi-ecosystem                  Git repo\n",
            " 6. asi-inference-protocol         Git repo\n",
            " 7. asi-protosymbiotic-signal      Git repo\n",
            " 8. asi-safeguards                 Git repo\n",
            " 9. asi-symbiotic-signal           Git repo\n",
            "10. attention-heatmap-visualizer   Git repo\n",
            "11. bias-reflector                 Git repo\n",
            "12. biosignal-translator           Git repo\n",
            "13. coevolutionary-loops           Git repo\n",
            "14. cognitive-engine               Git repo\n",
            "15. confidence-scorer              Git repo\n",
            "16. eco-benchmark                  Git repo\n",
            "17. eco-datacenter                 Git repo\n",
            "18. emergence-engine               Git repo\n",
            "19. healing-engine                 Git repo\n",
            "20. latent-memory                  Git repo\n",
            "21. mirror-aware-inference         Git repo\n",
            "22. ml-algorithm-dataset           Git repo\n",
            "23. ml-visual-engine               Git repo\n",
            "24. saliency-heatmap-visualizer    Git repo\n",
            "25. stigmergic-tracefinder         Git repo\n",
            "26. symbiotic-core-library         Git repo\n",
            "27. symbiotic-latent-memory        Git repo\n",
            "28. symbiotic-lexicon              Git repo\n",
            "29. thermo-adaptive-pipeline       Git repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Integrity Audit - Part II**"
      ],
      "metadata": {
        "id": "aydWI04IGlVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 8: Import Required Libraries for Integrity Verification\n",
        "print(\"Importing libraries for integrity verification...\")\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao3AZ1muGjWB",
        "outputId": "026d4bf6-641d-4e81-cc97-16f3cd4f5819"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing libraries for integrity verification...\n",
            "Libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Define Integrity Verification Functions\n",
        "print(\"Setting up integrity verification functions...\\n\")\n",
        "\n",
        "class IntegrityVerifier:\n",
        "    def __init__(self, repo_path):\n",
        "        self.repo_path = Path(repo_path)\n",
        "        self.repo_name = self.repo_path.name\n",
        "        self.results = {\n",
        "            'repo': self.repo_name,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'levels': {}\n",
        "        }\n",
        "\n",
        "    def run_git_command(self, cmd):\n",
        "        \"\"\"Execute git command and return output\"\"\"\n",
        "        result = subprocess.run(\n",
        "            cmd,\n",
        "            cwd=self.repo_path,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            shell=False\n",
        "        )\n",
        "        return result.stdout.strip(), result.stderr.strip(), result.returncode\n",
        "\n",
        "    def level_1_commit_and_tree_hash(self):\n",
        "        \"\"\"Level 1: Compare local vs remote commit and tree hashes\"\"\"\n",
        "        print(f\"  [Level 1] Commit & Tree Hash Comparison\")\n",
        "\n",
        "        # Get local commit hash\n",
        "        local_commit, _, ret1 = self.run_git_command(['git', 'rev-parse', 'HEAD'])\n",
        "\n",
        "        # Get local tree hash\n",
        "        local_tree, _, ret2 = self.run_git_command(['git', 'rev-parse', 'HEAD^{tree}'])\n",
        "\n",
        "        # Get remote commit hash\n",
        "        remote_info, _, ret3 = self.run_git_command(['git', 'ls-remote', 'origin', 'HEAD'])\n",
        "        remote_commit = remote_info.split()[0] if remote_info else None\n",
        "\n",
        "        if ret1 != 0 or ret2 != 0 or ret3 != 0:\n",
        "            self.results['levels']['level_1'] = {\n",
        "                'status': 'ERROR',\n",
        "                'message': 'Failed to retrieve git hashes'\n",
        "            }\n",
        "            print(f\"    [x] ERROR: Failed to retrieve git information\")\n",
        "            return False\n",
        "\n",
        "        commit_match = local_commit == remote_commit\n",
        "\n",
        "        self.results['levels']['level_1'] = {\n",
        "            'status': 'PASS' if commit_match else 'FAIL',\n",
        "            'local_commit': local_commit,\n",
        "            'remote_commit': remote_commit,\n",
        "            'local_tree': local_tree,\n",
        "            'commit_match': commit_match\n",
        "        }\n",
        "\n",
        "        if commit_match:\n",
        "            print(f\"    [âœ“] PASS - Commits match\")\n",
        "            print(f\"       Commit: {local_commit[:12]}...\")\n",
        "            print(f\"       Tree:   {local_tree[:12]}...\")\n",
        "        else:\n",
        "            print(f\"    [x] FAIL - Commit mismatch\")\n",
        "            print(f\"       Local:  {local_commit[:12]}...\")\n",
        "            print(f\"       Remote: {remote_commit[:12]}...\")\n",
        "\n",
        "        return commit_match\n",
        "\n",
        "    def level_2_git_fsck(self):\n",
        "        \"\"\"Level 2: Deep git repository integrity check\"\"\"\n",
        "        print(f\"  [Level 2] Git Repository Integrity (fsck)\")\n",
        "\n",
        "        output, stderr, returncode = self.run_git_command(['git', 'fsck', '--full'])\n",
        "\n",
        "        # git fsck returns 0 if no issues\n",
        "        passed = returncode == 0 and not any(word in output.lower() for word in ['error', 'missing', 'corrupt'])\n",
        "\n",
        "        self.results['levels']['level_2'] = {\n",
        "            'status': 'PASS' if passed else 'FAIL',\n",
        "            'returncode': returncode,\n",
        "            'issues_found': [] if passed else output.split('\\n')[:5]  # First 5 issues\n",
        "        }\n",
        "\n",
        "        if passed:\n",
        "            print(f\"    [âœ“] PASS - Repository integrity verified\")\n",
        "        else:\n",
        "            print(f\"    [x] FAIL - Repository integrity issues detected\")\n",
        "            if output:\n",
        "                print(f\"       Issues: {output[:100]}...\")\n",
        "\n",
        "        return passed\n",
        "\n",
        "    def level_3_file_hashing(self):\n",
        "        \"\"\"Level 3: File-by-file and folder structure verification\"\"\"\n",
        "        print(f\"  [Level 3] File-by-File Hash Verification\")\n",
        "\n",
        "        # Get list of all tracked files\n",
        "        files_output, _, ret = self.run_git_command(['git', 'ls-files'])\n",
        "\n",
        "        if ret != 0:\n",
        "            self.results['levels']['level_3'] = {\n",
        "                'status': 'ERROR',\n",
        "                'message': 'Failed to list git files'\n",
        "            }\n",
        "            print(f\"    [x] ERROR: Failed to list files\")\n",
        "            return False\n",
        "\n",
        "        files = files_output.split('\\n') if files_output else []\n",
        "\n",
        "        file_hashes = {}\n",
        "        corrupted_files = []\n",
        "        total_files = len(files)\n",
        "\n",
        "        # Calculate hash for each file\n",
        "        for file in files[:100]:  # Limit to first 100 files for performance\n",
        "            if not file:\n",
        "                continue\n",
        "            file_path = self.repo_path / file\n",
        "            if file_path.exists() and file_path.is_file():\n",
        "                try:\n",
        "                    with open(file_path, 'rb') as f:\n",
        "                        file_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "                        file_hashes[file] = file_hash\n",
        "\n",
        "                        # Verify against git's hash\n",
        "                        git_hash, _, ret = self.run_git_command(['git', 'hash-object', file])\n",
        "                        if ret != 0 or not git_hash:\n",
        "                            corrupted_files.append(file)\n",
        "                except Exception as e:\n",
        "                    corrupted_files.append(file)\n",
        "\n",
        "        passed = len(corrupted_files) == 0\n",
        "\n",
        "        self.results['levels']['level_3'] = {\n",
        "            'status': 'PASS' if passed else 'FAIL',\n",
        "            'total_files_checked': len(file_hashes),\n",
        "            'total_files_in_repo': total_files,\n",
        "            'corrupted_files': corrupted_files,\n",
        "            'sample_hashes': dict(list(file_hashes.items())[:3])  # First 3 as sample\n",
        "        }\n",
        "\n",
        "        if passed:\n",
        "            print(f\"    [âœ“] PASS - All files verified ({len(file_hashes)} checked)\")\n",
        "            if file_hashes:\n",
        "                sample_file = list(file_hashes.keys())[0]\n",
        "                print(f\"       Sample: {sample_file[:40]}... -> {file_hashes[sample_file][:12]}...\")\n",
        "        else:\n",
        "            print(f\"    [x] FAIL - {len(corrupted_files)} corrupted files detected\")\n",
        "            for cf in corrupted_files[:3]:\n",
        "                print(f\"       - {cf}\")\n",
        "\n",
        "        return passed\n",
        "\n",
        "    def level_4_tree_comparison(self):\n",
        "        \"\"\"Level 4: Complete tree hash comparison\"\"\"\n",
        "        print(f\"  [Level 4] Complete Tree Hash Verification\")\n",
        "\n",
        "        # Get local tree\n",
        "        local_tree, _, ret1 = self.run_git_command(['git', 'rev-parse', 'HEAD^{tree}'])\n",
        "\n",
        "        # Fetch latest from remote\n",
        "        _, _, ret2 = self.run_git_command(['git', 'fetch', 'origin', '--quiet'])\n",
        "\n",
        "        # Get remote tree\n",
        "        remote_tree, _, ret3 = self.run_git_command(['git', 'rev-parse', 'origin/HEAD^{tree}'])\n",
        "\n",
        "        if ret1 != 0 or ret3 != 0:\n",
        "            self.results['levels']['level_4'] = {\n",
        "                'status': 'ERROR',\n",
        "                'message': 'Failed to retrieve tree hashes'\n",
        "            }\n",
        "            print(f\"    [x] ERROR: Failed to retrieve tree information\")\n",
        "            return False\n",
        "\n",
        "        tree_match = local_tree == remote_tree\n",
        "\n",
        "        self.results['levels']['level_4'] = {\n",
        "            'status': 'PASS' if tree_match else 'FAIL',\n",
        "            'local_tree': local_tree,\n",
        "            'remote_tree': remote_tree,\n",
        "            'tree_match': tree_match\n",
        "        }\n",
        "\n",
        "        if tree_match:\n",
        "            print(f\"    [âœ“] PASS - Tree hashes match\")\n",
        "            print(f\"       Tree: {local_tree[:12]}...\")\n",
        "        else:\n",
        "            print(f\"    [x] FAIL - Tree hash mismatch\")\n",
        "            print(f\"       Local:  {local_tree[:12]}...\")\n",
        "            print(f\"       Remote: {remote_tree[:12]}...\")\n",
        "\n",
        "        return tree_match\n",
        "\n",
        "    def verify(self, levels=[1, 2, 3, 4]):\n",
        "        \"\"\"Run verification for specified levels\"\"\"\n",
        "        print(f\"\\n[VERIFYING] {self.repo_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        level_functions = {\n",
        "            1: self.level_1_commit_and_tree_hash,\n",
        "            2: self.level_2_git_fsck,\n",
        "            3: self.level_3_file_hashing,\n",
        "            4: self.level_4_tree_comparison\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "        for level in sorted(levels):\n",
        "            if level in level_functions:\n",
        "                results[level] = level_functions[level]()\n",
        "            else:\n",
        "                print(f\"  [!] Warning: Level {level} not recognized\")\n",
        "\n",
        "        # Overall status\n",
        "        all_passed = all(results.values())\n",
        "        self.results['overall_status'] = 'PASS' if all_passed else 'FAIL'\n",
        "\n",
        "        status_symbol = \"[âœ“]\" if all_passed else \"[x]\"\n",
        "        print(f\"\\n  {status_symbol} Overall: {'PASS' if all_passed else 'FAIL'}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "print(\"[âœ“] Integrity verification functions ready\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu4nJxM-H14H",
        "outputId": "e0f74ec9-e2bb-4870-f2cb-57dfd485a0d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up integrity verification functions...\n",
            "\n",
            "[âœ“] Integrity verification functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Configure Integrity Verification Levels\n",
        "print(\"Integrity Verification Configuration\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAvailable verification levels:\")\n",
        "print(\"  Level 1: Commit & Tree Hash Comparison (Fast)\")\n",
        "print(\"  Level 2: Git Repository Integrity Check (Medium)\")\n",
        "print(\"  Level 3: File-by-File Hash Verification (Slow)\")\n",
        "print(\"  Level 4: Complete Tree Hash Comparison (Fast)\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# USER CONFIGURATION: Choose which levels to run\n",
        "VERIFICATION_LEVELS = [1, 2, 3, 4]  # Modify this list to choose levels\n",
        "\n",
        "print(f\"\\n[âœ“] Running levels: {VERIFICATION_LEVELS}\")\n",
        "print(f\"[âœ“] This will verify all repositories in the 'repositories' folder\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QatMkQH4H5rR",
        "outputId": "afd41070-c849-4157-c50b-d4a4db89bcde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrity Verification Configuration\n",
            "============================================================\n",
            "\n",
            "Available verification levels:\n",
            "  Level 1: Commit & Tree Hash Comparison (Fast)\n",
            "  Level 2: Git Repository Integrity Check (Medium)\n",
            "  Level 3: File-by-File Hash Verification (Slow)\n",
            "  Level 4: Complete Tree Hash Comparison (Fast)\n",
            "\n",
            "============================================================\n",
            "\n",
            "[âœ“] Running levels: [1, 2, 3, 4]\n",
            "[âœ“] This will verify all repositories in the 'repositories' folder\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Run Integrity Verification on All Repositories\n",
        "print(\"Starting Integrity Verification Process\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "repositories_path = Path('/content/asi-ecosystem/repositories')\n",
        "\n",
        "if not repositories_path.exists():\n",
        "    print(\"[x] Error: repositories folder not found!\")\n",
        "else:\n",
        "    repos = [d for d in repositories_path.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
        "\n",
        "    print(f\"Found {len(repos)} repositories to verify\\n\")\n",
        "\n",
        "    all_results = []\n",
        "    verification_start = datetime.now()\n",
        "\n",
        "    for i, repo_path in enumerate(sorted(repos), 1):\n",
        "        print(f\"\\n[{i}/{len(repos)}]\")\n",
        "        verifier = IntegrityVerifier(repo_path)\n",
        "        result = verifier.verify(levels=VERIFICATION_LEVELS)\n",
        "        all_results.append(result)\n",
        "\n",
        "    verification_end = datetime.now()\n",
        "    duration = (verification_end - verification_start).total_seconds()\n",
        "\n",
        "    # Summary Statistics\n",
        "    print(\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"VERIFICATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    passed = sum(1 for r in all_results if r['overall_status'] == 'PASS')\n",
        "    failed = len(all_results) - passed\n",
        "\n",
        "    print(f\"\\n[âœ“] Total Repositories: {len(all_results)}\")\n",
        "    print(f\"[âœ“] Passed: {passed}\")\n",
        "    print(f\"[âœ“] Failed: {failed}\")\n",
        "    print(f\"[âœ“] Duration: {duration:.2f} seconds\")\n",
        "\n",
        "    # Level-by-level summary\n",
        "    print(f\"\\nLevel-by-Level Results:\")\n",
        "    for level in VERIFICATION_LEVELS:\n",
        "        level_passed = sum(1 for r in all_results\n",
        "                          if f'level_{level}' in r['levels']\n",
        "                          and r['levels'][f'level_{level}']['status'] == 'PASS')\n",
        "        level_total = sum(1 for r in all_results if f'level_{level}' in r['levels'])\n",
        "        print(f\"   Level {level}: {level_passed}/{level_total} passed\")\n",
        "\n",
        "    # Failed repositories detail\n",
        "    if failed > 0:\n",
        "        print(f\"\\nFailed Repositories:\")\n",
        "        for result in all_results:\n",
        "            if result['overall_status'] == 'FAIL':\n",
        "                print(f\"\\n   [x] {result['repo']}\")\n",
        "                for level_key, level_data in result['levels'].items():\n",
        "                    if level_data['status'] != 'PASS':\n",
        "                        level_num = level_key.replace('level_', '')\n",
        "                        print(f\"      - Level {level_num}: {level_data['status']}\")\n",
        "                        if 'message' in level_data:\n",
        "                            print(f\"        {level_data['message']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"[âœ“] Integrity verification complete!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Store results for optional export\n",
        "    integrity_results = {\n",
        "        'verification_date': verification_start.isoformat(),\n",
        "        'duration_seconds': duration,\n",
        "        'levels_checked': VERIFICATION_LEVELS,\n",
        "        'summary': {\n",
        "            'total': len(all_results),\n",
        "            'passed': passed,\n",
        "            'failed': failed\n",
        "        },\n",
        "        'repositories': all_results\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy5vjvLcIKWB",
        "outputId": "d9d871fa-3b61-4261-83ed-2c1c9e1f7b49"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Integrity Verification Process\n",
            "============================================================\n",
            "\n",
            "Found 29 repositories to verify\n",
            "\n",
            "\n",
            "[1/29]\n",
            "\n",
            "[VERIFYING] active-learning-dataset\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 058f01ab1803...\n",
            "       Tree:   4dac4b7fcdb4...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (100 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 4dac4b7fcdb4...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[2/29]\n",
            "\n",
            "[VERIFYING] asi-backups\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 1f0099675c7f...\n",
            "       Tree:   1b82412566ab...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (100 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 1b82412566ab...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[3/29]\n",
            "\n",
            "[VERIFYING] asi-core-protocol\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: ae2a7c5e4eda...\n",
            "       Tree:   788372a9a101...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (8 checked)\n",
            "       Sample: ASI_Core_Protocol.json... -> 8836cafc1e79...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 788372a9a101...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[4/29]\n",
            "\n",
            "[VERIFYING] asi-dynamic-core\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: bb198611cffb...\n",
            "       Tree:   e4d2cbfc39c0...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: e4d2cbfc39c0...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[5/29]\n",
            "\n",
            "[VERIFYING] asi-ecosystem\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: a779d1677b51...\n",
            "       Tree:   95a13d612d67...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (52 checked)\n",
            "       Sample: .github/log.txt... -> 01ba4719c80b...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 95a13d612d67...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[6/29]\n",
            "\n",
            "[VERIFYING] asi-inference-protocol\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 653f9f234897...\n",
            "       Tree:   0a0342f79124...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 0a0342f79124...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[7/29]\n",
            "\n",
            "[VERIFYING] asi-protosymbiotic-signal\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 188608d11a24...\n",
            "       Tree:   e6ca33cadeee...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (15 checked)\n",
            "       Sample: Cargo.toml... -> d9ad5837a11f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: e6ca33cadeee...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[8/29]\n",
            "\n",
            "[VERIFYING] asi-safeguards\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 4c9b4e760751...\n",
            "       Tree:   c781276a9fe6...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (4 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: c781276a9fe6...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[9/29]\n",
            "\n",
            "[VERIFYING] asi-symbiotic-signal\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: b84968d5898c...\n",
            "       Tree:   920a28bae4b3...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 920a28bae4b3...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[10/29]\n",
            "\n",
            "[VERIFYING] attention-heatmap-visualizer\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: a48396a4ffd7...\n",
            "       Tree:   79fff2828ad3...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (15 checked)\n",
            "       Sample: .github/CODEOWNERS... -> b39eb235c951...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 79fff2828ad3...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[11/29]\n",
            "\n",
            "[VERIFYING] bias-reflector\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: d177bca8c6d8...\n",
            "       Tree:   739ae837469a...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (3 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 739ae837469a...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[12/29]\n",
            "\n",
            "[VERIFYING] biosignal-translator\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 075c56e4ca1a...\n",
            "       Tree:   065c8518163b...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 065c8518163b...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[13/29]\n",
            "\n",
            "[VERIFYING] coevolutionary-loops\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 66f0fc37c8be...\n",
            "       Tree:   e9d61bbc3337...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (3 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: e9d61bbc3337...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[14/29]\n",
            "\n",
            "[VERIFYING] cognitive-engine\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 32223e5ceaec...\n",
            "       Tree:   c6a842e15cd4...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: c6a842e15cd4...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[15/29]\n",
            "\n",
            "[VERIFYING] confidence-scorer\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 1ed06c2a9298...\n",
            "       Tree:   ac3527e9c8b7...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (26 checked)\n",
            "       Sample: .github/CODEOWNERS... -> 24d2241d0440...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: ac3527e9c8b7...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[16/29]\n",
            "\n",
            "[VERIFYING] eco-benchmark\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: d86327146b47...\n",
            "       Tree:   d0d581f13b1f...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (4 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: d0d581f13b1f...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[17/29]\n",
            "\n",
            "[VERIFYING] eco-datacenter\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: b87fd7d9aee3...\n",
            "       Tree:   aee35b2ea9cc...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: aee35b2ea9cc...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[18/29]\n",
            "\n",
            "[VERIFYING] emergence-engine\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 033309086e0a...\n",
            "       Tree:   37aaa9eef04d...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (12 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 37aaa9eef04d...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[19/29]\n",
            "\n",
            "[VERIFYING] healing-engine\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: b1811a934081...\n",
            "       Tree:   94936cbefccf...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (5 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 94936cbefccf...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[20/29]\n",
            "\n",
            "[VERIFYING] latent-memory\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 510aff82670a...\n",
            "       Tree:   22d997399900...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (39 checked)\n",
            "       Sample: .github/CODEOWNERS... -> 10090a66df19...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 22d997399900...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[21/29]\n",
            "\n",
            "[VERIFYING] mirror-aware-inference\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: efe8a4fac8ff...\n",
            "       Tree:   a61c81cff5a6...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (3 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: a61c81cff5a6...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[22/29]\n",
            "\n",
            "[VERIFYING] ml-algorithm-dataset\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: d58f25d589f3...\n",
            "       Tree:   4617c946b50b...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (7 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 4617c946b50b...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[23/29]\n",
            "\n",
            "[VERIFYING] ml-visual-engine\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: ef918bd370c0...\n",
            "       Tree:   924cd0d454f8...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (97 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 924cd0d454f8...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[24/29]\n",
            "\n",
            "[VERIFYING] saliency-heatmap-visualizer\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: fc7a2cae8665...\n",
            "       Tree:   cea475ec292c...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (9 checked)\n",
            "       Sample: .github/CODEOWNERS... -> 6120c420733e...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: cea475ec292c...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[25/29]\n",
            "\n",
            "[VERIFYING] stigmergic-tracefinder\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 627dacdee88a...\n",
            "       Tree:   556dad48c8fe...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 556dad48c8fe...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[26/29]\n",
            "\n",
            "[VERIFYING] symbiotic-core-library\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: c84c52c968d5...\n",
            "       Tree:   3d792fcbae9b...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (11 checked)\n",
            "       Sample: .github/CODEOWNERS... -> 6255024a49aa...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 3d792fcbae9b...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[27/29]\n",
            "\n",
            "[VERIFYING] symbiotic-latent-memory\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 686fd26e4791...\n",
            "       Tree:   a26974d11b18...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (3 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: a26974d11b18...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[28/29]\n",
            "\n",
            "[VERIFYING] symbiotic-lexicon\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: f14817e7e0f0...\n",
            "       Tree:   661fa60b37a3...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 661fa60b37a3...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "[29/29]\n",
            "\n",
            "[VERIFYING] thermo-adaptive-pipeline\n",
            "============================================================\n",
            "  [Level 1] Commit & Tree Hash Comparison\n",
            "    [âœ“] PASS - Commits match\n",
            "       Commit: 197a021a4d6f...\n",
            "       Tree:   32b8e3565349...\n",
            "  [Level 2] Git Repository Integrity (fsck)\n",
            "    [âœ“] PASS - Repository integrity verified\n",
            "  [Level 3] File-by-File Hash Verification\n",
            "    [âœ“] PASS - All files verified (2 checked)\n",
            "       Sample: LICENSE... -> 2a96460bba8f...\n",
            "  [Level 4] Complete Tree Hash Verification\n",
            "    [âœ“] PASS - Tree hashes match\n",
            "       Tree: 32b8e3565349...\n",
            "\n",
            "  [âœ“] Overall: PASS\n",
            "\n",
            "\n",
            "============================================================\n",
            "VERIFICATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "[âœ“] Total Repositories: 29\n",
            "[âœ“] Passed: 29\n",
            "[âœ“] Failed: 0\n",
            "[âœ“] Duration: 20.32 seconds\n",
            "\n",
            "Level-by-Level Results:\n",
            "   Level 1: 29/29 passed\n",
            "   Level 2: 29/29 passed\n",
            "   Level 3: 29/29 passed\n",
            "   Level 4: 29/29 passed\n",
            "\n",
            "============================================================\n",
            "[âœ“] Integrity verification complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Optional - Export Detailed Report to JSON\n",
        "print(\"\\nExport Options\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "export_report = True  # Set to True to export JSON report\n",
        "\n",
        "if export_report:\n",
        "    report_path = '/content/integrity_report.json'\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(integrity_results, f, indent=2)\n",
        "\n",
        "    print(f\"[âœ“] Detailed report exported to: {report_path}\")\n",
        "    print(f\"Report size: {Path(report_path).stat().st_size / 1024:.2f} KB\")\n",
        "\n",
        "    # Show sample of report structure\n",
        "    print(\"\\nReport structure preview:\")\n",
        "    print(f\"   - Verification date: {integrity_results['verification_date']}\")\n",
        "    print(f\"   - Total repositories: {integrity_results['summary']['total']}\")\n",
        "    print(f\"   - Levels checked: {integrity_results['levels_checked']}\")\n",
        "    print(f\"   - Detailed results: {len(integrity_results['repositories'])} entries\")\n",
        "else:\n",
        "    print(\"[i] Report export disabled (set export_report=True to enable)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s1iZFyOIO6C",
        "outputId": "2a80d7d5-63ed-4d5c-f762-21e0413daf6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Export Options\n",
            "============================================================\n",
            "[âœ“] Detailed report exported to: /content/integrity_report.json\n",
            "Report size: 36.23 KB\n",
            "\n",
            "Report structure preview:\n",
            "   - Verification date: 2025-12-14T13:48:48.849910\n",
            "   - Total repositories: 29\n",
            "   - Levels checked: [1, 2, 3, 4]\n",
            "   - Detailed results: 29 entries\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation - Part III**"
      ],
      "metadata": {
        "id": "-uFcuRn4ZKv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Configure Dataset Creation Parameters\n",
        "print(\"Configuring dataset creation parameters...\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration ---\n",
        "REPOSITORIES_SRC_DIR = Path('/content/asi-ecosystem/repositories')\n",
        "OUTPUT_DATASET_FILE = Path('/content/dataset.txt')\n",
        "EXCLUDED_DIRS = ['.git']\n",
        "# Attempt to process only files with these extensions. Leave empty to try all.\n",
        "INCLUDED_EXTENSIONS = [\n",
        "    # Code\n",
        "    '.py', '.rs', '.js', '.ts', '.java', '.c', '.h', '.cpp', '.go', '.sh',\n",
        "    # Config/Data\n",
        "    '.json', '.yaml', '.yml', '.toml', '.xml', '.ini',\n",
        "    # Docs\n",
        "    '.md', '.txt', '.rst'\n",
        "]\n",
        "# Files with extensions not in the list above will be skipped.\n",
        "\n",
        "# --- Special tokens for structuring the dataset ---\n",
        "REPO_START_TOKEN = \"<|repo_start|>\"\n",
        "REPO_END_TOKEN = \"<|repo_end|>\"\n",
        "FILE_START_TOKEN = \"<|file_start|>\"\n",
        "FILE_END_TOKEN = \"<|file_end|>\"\n",
        "\n",
        "print(f\"Source directory: {REPOSITORIES_SRC_DIR}\")\n",
        "print(f\"Output file: {OUTPUT_DATASET_FILE}\")\n",
        "print(f\"Excluded directories: {EXCLUDED_DIRS}\")\n",
        "print(f\"Included extensions: {len(INCLUDED_EXTENSIONS)} types\")\n",
        "print(\"\\n[âœ“] Configuration complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-BYx3aIZQEA",
        "outputId": "02cca13a-09bd-4c0b-ca50-d25fbe64169e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring dataset creation parameters...\n",
            "Source directory: /content/asi-ecosystem/repositories\n",
            "Output file: /content/dataset.txt\n",
            "Excluded directories: ['.git']\n",
            "Included extensions: 19 types\n",
            "\n",
            "[âœ“] Configuration complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Process Repositories and Build the Dataset File\n",
        "print(\"Starting dataset creation process...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "processed_files_count = 0\n",
        "processed_repos_count = 0\n",
        "skipped_files_count = 0\n",
        "\n",
        "# Get a list of repository directories\n",
        "if not REPOSITORIES_SRC_DIR.exists():\n",
        "    print(f\"[x] ERROR: Source directory not found at '{REPOSITORIES_SRC_DIR}'\")\n",
        "else:\n",
        "    # --- Curriculum Learning Order ---\n",
        "    # Define the specific processing order for repositories\n",
        "    priority_order = [\n",
        "        'asi-ecosystem',\n",
        "        'symbiotic-core-library',\n",
        "        'asi-protosymbiotic-signal',\n",
        "        'asi-symbiotic-signal',\n",
        "        'asi-core-protocol',\n",
        "        'eco-benchmark',\n",
        "        'eco-datacenter'\n",
        "    ]\n",
        "    last_order = [\n",
        "        'asi-backups'\n",
        "        'emergence-engine',\n",
        "    ]\n",
        "\n",
        "    all_repos_on_disk = {d.name: d for d in REPOSITORIES_SRC_DIR.iterdir() if d.is_dir()}\n",
        "    sorted_repo_paths = []\n",
        "\n",
        "    # 1. Add priority repos in their specified order\n",
        "    for repo_name in priority_order:\n",
        "        if repo_name in all_repos_on_disk:\n",
        "            sorted_repo_paths.append(all_repos_on_disk.pop(repo_name))\n",
        "\n",
        "    # 2. Add the remaining repos (alphabetically), excluding the ones for the end\n",
        "    middle_repos_names = sorted([\n",
        "        name for name in all_repos_on_disk\n",
        "        if name not in last_order\n",
        "    ])\n",
        "    for repo_name in middle_repos_names:\n",
        "        sorted_repo_paths.append(all_repos_on_disk.pop(repo_name))\n",
        "\n",
        "    # 3. Add the last repos in their specified order\n",
        "    for repo_name in last_order:\n",
        "        if repo_name in all_repos_on_disk:\n",
        "            sorted_repo_paths.append(all_repos_on_disk.pop(repo_name))\n",
        "\n",
        "    print(f\"Found {len(sorted_repo_paths)} repositories to process in curriculum order.\")\n",
        "\n",
        "    with open(OUTPUT_DATASET_FILE, 'w', encoding='utf-8') as outfile:\n",
        "        # Use the new custom-sorted list of repository paths\n",
        "        for repo_path in sorted_repo_paths:\n",
        "            repo_name = repo_path.name\n",
        "            print(f\"\\n[Processing] '{repo_name}'...\")\n",
        "            processed_repos_count += 1\n",
        "\n",
        "            # Write the repository start token and its name\n",
        "            outfile.write(f\"{REPO_START_TOKEN}{repo_name}\\n\")\n",
        "\n",
        "            # Use rglob to recursively find all files\n",
        "            files_in_repo = list(repo_path.rglob('*'))\n",
        "            print(f\"  Found {len(files_in_repo)} total items (files/dirs). Filtering...\")\n",
        "\n",
        "            repo_file_count = 0\n",
        "            for file_path in files_in_repo:\n",
        "                # Skip directories and files in excluded directories\n",
        "                if not file_path.is_file() or any(d in file_path.parts for d in EXCLUDED_DIRS):\n",
        "                    continue\n",
        "\n",
        "                # Filter by extension if the list is not empty\n",
        "                if INCLUDED_EXTENSIONS and file_path.suffix.lower() not in INCLUDED_EXTENSIONS:\n",
        "                    skipped_files_count += 1\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Get relative path to store in the dataset\n",
        "                    relative_path = file_path.relative_to(repo_path)\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:\n",
        "                        content = infile.read()\n",
        "\n",
        "                    # Write the file start token and its path\n",
        "                    outfile.write(f\"{FILE_START_TOKEN}{relative_path}\\n\")\n",
        "                    # Write the file content\n",
        "                    outfile.write(content)\n",
        "                    # Write the file end token\n",
        "                    outfile.write(f\"\\n{FILE_END_TOKEN}\\n\")\n",
        "\n",
        "                    processed_files_count += 1\n",
        "                    repo_file_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  [!] Warning: Could not process file {file_path}. Reason: {e}\")\n",
        "                    skipped_files_count += 1\n",
        "\n",
        "            print(f\"  -> Added content from {repo_file_count} files.\")\n",
        "\n",
        "            # Write the repository end token\n",
        "            outfile.write(f\"{REPO_END_TOKEN}\\n\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Dataset Creation Summary\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  Total repositories processed: {processed_repos_count}\")\n",
        "    print(f\"  Total text files added: {processed_files_count}\")\n",
        "    print(f\"  Total files skipped (binary/extension/error): {skipped_files_count}\")\n",
        "    print(f\"\\n[âœ“] Dataset successfully created at: {OUTPUT_DATASET_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da2QmCeSZSlz",
        "outputId": "5f1df41e-b744-4e8a-9cd0-0caebe598d87"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset creation process...\n",
            "============================================================\n",
            "Found 29 repositories to process in curriculum order.\n",
            "\n",
            "[Processing] 'asi-ecosystem'...\n",
            "  Found 133 total items (files/dirs). Filtering...\n",
            "  -> Added content from 49 files.\n",
            "\n",
            "[Processing] 'symbiotic-core-library'...\n",
            "  Found 57 total items (files/dirs). Filtering...\n",
            "  -> Added content from 8 files.\n",
            "\n",
            "[Processing] 'asi-protosymbiotic-signal'...\n",
            "  Found 61 total items (files/dirs). Filtering...\n",
            "  -> Added content from 10 files.\n",
            "\n",
            "[Processing] 'asi-symbiotic-signal'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'asi-core-protocol'...\n",
            "  Found 53 total items (files/dirs). Filtering...\n",
            "  -> Added content from 6 files.\n",
            "\n",
            "[Processing] 'eco-benchmark'...\n",
            "  Found 48 total items (files/dirs). Filtering...\n",
            "  -> Added content from 3 files.\n",
            "\n",
            "[Processing] 'eco-datacenter'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'active-learning-dataset'...\n",
            "  Found 177 total items (files/dirs). Filtering...\n",
            "  -> Added content from 17 files.\n",
            "\n",
            "[Processing] 'asi-backups'...\n",
            "  Found 2598 total items (files/dirs). Filtering...\n",
            "  -> Added content from 2011 files.\n",
            "\n",
            "[Processing] 'asi-dynamic-core'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'asi-inference-protocol'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'asi-safeguards'...\n",
            "  Found 48 total items (files/dirs). Filtering...\n",
            "  -> Added content from 3 files.\n",
            "\n",
            "[Processing] 'attention-heatmap-visualizer'...\n",
            "  Found 60 total items (files/dirs). Filtering...\n",
            "  -> Added content from 6 files.\n",
            "\n",
            "[Processing] 'bias-reflector'...\n",
            "  Found 47 total items (files/dirs). Filtering...\n",
            "  -> Added content from 2 files.\n",
            "\n",
            "[Processing] 'biosignal-translator'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'coevolutionary-loops'...\n",
            "  Found 47 total items (files/dirs). Filtering...\n",
            "  -> Added content from 2 files.\n",
            "\n",
            "[Processing] 'cognitive-engine'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'confidence-scorer'...\n",
            "  Found 75 total items (files/dirs). Filtering...\n",
            "  -> Added content from 16 files.\n",
            "\n",
            "[Processing] 'emergence-engine'...\n",
            "  Found 57 total items (files/dirs). Filtering...\n",
            "  -> Added content from 11 files.\n",
            "\n",
            "[Processing] 'healing-engine'...\n",
            "  Found 49 total items (files/dirs). Filtering...\n",
            "  -> Added content from 4 files.\n",
            "\n",
            "[Processing] 'latent-memory'...\n",
            "  Found 88 total items (files/dirs). Filtering...\n",
            "  -> Added content from 36 files.\n",
            "\n",
            "[Processing] 'mirror-aware-inference'...\n",
            "  Found 49 total items (files/dirs). Filtering...\n",
            "  -> Added content from 2 files.\n",
            "\n",
            "[Processing] 'ml-algorithm-dataset'...\n",
            "  Found 51 total items (files/dirs). Filtering...\n",
            "  -> Added content from 5 files.\n",
            "\n",
            "[Processing] 'ml-visual-engine'...\n",
            "  Found 146 total items (files/dirs). Filtering...\n",
            "  -> Added content from 85 files.\n",
            "\n",
            "[Processing] 'saliency-heatmap-visualizer'...\n",
            "  Found 55 total items (files/dirs). Filtering...\n",
            "  -> Added content from 5 files.\n",
            "\n",
            "[Processing] 'stigmergic-tracefinder'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'symbiotic-latent-memory'...\n",
            "  Found 47 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'symbiotic-lexicon'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "[Processing] 'thermo-adaptive-pipeline'...\n",
            "  Found 46 total items (files/dirs). Filtering...\n",
            "  -> Added content from 1 files.\n",
            "\n",
            "============================================================\n",
            "Dataset Creation Summary\n",
            "============================================================\n",
            "  Total repositories processed: 29\n",
            "  Total text files added: 2291\n",
            "  Total files skipped (binary/extension/error): 492\n",
            "\n",
            "[âœ“] Dataset successfully created at: /content/dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Verify and Preview the Created Dataset\n",
        "print(\"Verifying the created dataset file...\")\n",
        "\n",
        "if not OUTPUT_DATASET_FILE.exists():\n",
        "    print(f\"[x] ERROR: Dataset file '{OUTPUT_DATASET_FILE}' not found!\")\n",
        "else:\n",
        "    file_size_kb = OUTPUT_DATASET_FILE.stat().st_size / 1024\n",
        "    print(f\"[âœ“] Dataset file found.\")\n",
        "    print(f\"    - Size: {file_size_kb:.2f} KB\")\n",
        "\n",
        "    print(\"\\n--- Preview of the first 1000 characters ---\")\n",
        "    with open(OUTPUT_DATASET_FILE, 'r', encoding='utf-8') as f:\n",
        "        preview_content = f.read(1000)\n",
        "        print(preview_content)\n",
        "    print(\"--- End of Preview ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ohy6nzUZVsE",
        "outputId": "959d0ee5-eebc-419f-f602-a2c3653306c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying the created dataset file...\n",
            "[âœ“] Dataset file found.\n",
            "    - Size: 28037.21 KB\n",
            "\n",
            "--- Preview of the first 1000 characters ---\n",
            "<|repo_start|>asi-ecosystem\n",
            "<|file_start|>requirements.txt\n",
            "\n",
            "\n",
            "<|file_end|>\n",
            "<|file_start|>ecosystem_integration.md\n",
            "# Ecosystem Integration\n",
            "\n",
            "A system to seamlessly combine all the separate parts of the project into one cohesive local workspace.\n",
            "\n",
            "## Repository Structure\n",
            "\n",
            "```\n",
            "asi-ecosystem/\n",
            "â”œâ”€â”€ README.md\n",
            "â”œâ”€â”€ requirements.txt\n",
            "â”œâ”€â”€ docker-pipeline.md\n",
            "â”œâ”€â”€ ecosystem_integration.md\n",
            "â”œâ”€â”€ ecosystem_integration.ipynb\n",
            "â””â”€â”€ scripts/\n",
            "    â””â”€â”€ clone_ecosystem.sh\n",
            "    â””â”€â”€ docker_pipeline/\n",
            "       â”œâ”€â”€ Dockerfile\n",
            "       â”œâ”€â”€ requirements.txt\n",
            "       â”œâ”€â”€ run_ecosystem_pipeline.py\n",
            "       â”œâ”€â”€ phase1_cloning.py\n",
            "       â”œâ”€â”€ phase2_integrity.py\n",
            "       â”œâ”€â”€ phase3_dataset.py\n",
            "       â””â”€â”€ start.sh\n",
            "â””â”€â”€ repositories/    \n",
            "```\n",
            "\n",
            "## Ecosystem Integration Scripts and Workflows\n",
            "\n",
            "In addition to the hub's organizational structure, I am now incorporating scripts and workflows to integrate its intended functions into the existing information ecosystem.\n",
            "\n",
            "# 1. Automated ASI Ecosystem Integration - Google Colab Notebook\n",
            "\n",
            "The provided Googl\n",
            "--- End of Preview ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Load the Dataset into a Variable\n",
        "print(f\"Loading dataset from '{OUTPUT_DATASET_FILE}' into memory...\")\n",
        "\n",
        "training_data = \"\"\n",
        "\n",
        "try:\n",
        "    with open(OUTPUT_DATASET_FILE, 'r', encoding='utf-8') as f:\n",
        "        training_data = f.read()\n",
        "\n",
        "    print(\"\\n[âœ“] Success! The dataset is now loaded into the 'training_data' variable.\")\n",
        "    print(f\"    - Type: {type(training_data)}\")\n",
        "    print(f\"    - Total characters: {len(training_data)}\")\n",
        "    print(\"\\nThis variable can now be used as input for a tokenizer and training script.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"[x] ERROR: The file could not be found. Please run the previous cells first.\")\n",
        "except Exception as e:\n",
        "    print(f\"[x] ERROR: An unexpected error occurred while loading the file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN5Ftg65ZY6v",
        "outputId": "c75fe5f6-3830-476d-b201-6e8fe4c57f59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from '/content/dataset.txt' into memory...\n",
            "\n",
            "[âœ“] Success! The dataset is now loaded into the 'training_data' variable.\n",
            "    - Type: <class 'str'>\n",
            "    - Total characters: 28432527\n",
            "\n",
            "This variable can now be used as input for a tokenizer and training script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31f753b5",
        "outputId": "44a6d62d-9e9d-455c-88cb-40b17e087675"
      },
      "source": [
        "# Cell 17: Save as .txt if needed\n",
        "import os\n",
        "\n",
        "# Define the output path for the final training data file\n",
        "FINAL_TRAINING_DATA_FILE = '/content/training_dataset_final.txt'\n",
        "\n",
        "print(f\"Saving training data to '{FINAL_TRAINING_DATA_FILE}'...\")\n",
        "\n",
        "try:\n",
        "    with open(FINAL_TRAINING_DATA_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(training_data)\n",
        "    print(f\"[âœ“] Training data successfully saved to '{FINAL_TRAINING_DATA_FILE}'.\")\n",
        "    print(f\"    - Size: {os.path.getsize(FINAL_TRAINING_DATA_FILE) / (1024 * 1024):.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"[x] ERROR: Could not save training data: {e}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training data to '/content/training_dataset_final.txt'...\n",
            "[âœ“] Training data successfully saved to '/content/training_dataset_final.txt'.\n",
            "    - Size: 27.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87b4f711",
        "outputId": "a032942c-588e-4c6b-aacb-a69cd41beb39"
      },
      "source": [
        "# Cell 18: Save .txt on drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the destination path in Google Drive\n",
        "# You can change 'Colab Notebooks' to any folder in your Drive\n",
        "DRIVE_DEST_PATH = '/content/drive/MyDrive/Colab Notebooks/training_dataset_final.txt'\n",
        "\n",
        "print(f\"Copying '{FINAL_TRAINING_DATA_FILE}' to '{DRIVE_DEST_PATH}'...\")\n",
        "\n",
        "try:\n",
        "    shutil.copy(FINAL_TRAINING_DATA_FILE, DRIVE_DEST_PATH)\n",
        "    print(f\"[âœ“] File successfully copied to Google Drive: {DRIVE_DEST_PATH}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"[x] ERROR: Source file '{FINAL_TRAINING_DATA_FILE}' not found. Please ensure Cell 17 ran successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"[x] ERROR: Could not copy file to Google Drive. Ensure Drive is mounted and path exists: {e}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Copying '/content/training_dataset_final.txt' to '/content/drive/MyDrive/Colab Notebooks/training_dataset_final.txt'...\n",
            "[âœ“] File successfully copied to Google Drive: /content/drive/MyDrive/Colab Notebooks/training_dataset_final.txt\n"
          ]
        }
      ]
    }
  ]
}
