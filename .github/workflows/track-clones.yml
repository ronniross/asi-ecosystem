# This new logic abandons the "sliding window differential" method. Instead, it treats the clone data as a timeline.
# First Run: It ingests the entire clones array provided by GitHub (the last 14 days of history) and saves the latest timestamp seen.
# Daily Runs: It checks the API's clones array, filters for any dates newer than the last saved timestamp (usually just "yesterday"), and adds only those new numbers to your running total.

name: Track All Repository Clones

on:
  schedule:
    - cron: '33 3 * * *'
  workflow_dispatch:
  push:
    paths:
      - '.github/workflows/track-clones.yml'
      - 'README.md'

jobs:
  track-clones:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Initialize tracking system
        run: |
          mkdir -p analytics
          # Ensure ecosystem state file exists
          if [ ! -f analytics/ecosystem_state.json ]; then
            echo '{"total_clones": 0, "unique_clones": 0, "last_updated": null}' > analytics/ecosystem_state.json
          fi

      - name: Extract repositories from README
        run: |
          grep -oP 'https://github\.com/\K[\w-]+/[\w.-]+' README.md | sort -u > extracted_repos.txt
          echo "Extracted repositories:"
          cat extracted_repos.txt
          echo "---"

      - name: Fetch and record clone data
        env:
          TRAFFIC_TOKEN: ${{ secrets.TRAFFIC_TOKEN }}
        run: |
          TIMESTAMP=$(TZ='Etc/GMT+3' date +%Y-%m-%d_%H-%M-%S)
          READABLE_TIMESTAMP=$(TZ='Etc/GMT+3' date '+%Y-%m-%d %H:%M:%S GMT-3')
          
          # Initialize ecosystem counters for this run
          ECOSYSTEM_TOTAL=0
          ECOSYSTEM_UNIQUE=0
          
          echo "Starting processing..."
          
          while IFS= read -r REPO_FULL_NAME || [ -n "$REPO_FULL_NAME" ]; do
            [[ -z "$REPO_FULL_NAME" ]] && continue
            REPO_FULL_NAME=$(echo "$REPO_FULL_NAME" | sed 's/[./]*$//')
            REPO_NAME="${REPO_FULL_NAME#*/}"
            
            echo "Processing: $REPO_FULL_NAME"
            mkdir -p "analytics/$REPO_NAME"
            
            # 1. Fetch API Data
            HTTP_RESPONSE=$(curl -s -w "%{http_code}" -H "Authorization: token $TRAFFIC_TOKEN" "https://api.github.com/repos/$REPO_FULL_NAME/traffic/clones")
            HTTP_BODY=${HTTP_RESPONSE:0:${#HTTP_RESPONSE}-3}
            HTTP_STATUS=${HTTP_RESPONSE:${#HTTP_RESPONSE}-3}
            
            if [ "$HTTP_STATUS" != "200" ]; then
                echo "::error::Failed to fetch $REPO_FULL_NAME. Status: $HTTP_STATUS"
                continue
            fi
            
            # 2. Load Previous State
            STATE_FILE="analytics/$REPO_NAME/state.json"
            if [ -f "$STATE_FILE" ]; then
                PREV_LAST_TS=$(jq -r '.last_tracked_timestamp // empty' "$STATE_FILE")
                CUMULATIVE_TOTAL=$(jq '.cumulative_total // 0' "$STATE_FILE")
                CUMULATIVE_UNIQUE=$(jq '.cumulative_unique // 0' "$STATE_FILE")
                echo "  > Loaded state: Last tracked $PREV_LAST_TS | Current Total: $CUMULATIVE_TOTAL"
            else
                PREV_LAST_TS=""
                CUMULATIVE_TOTAL=0
                CUMULATIVE_UNIQUE=0
                echo "  > First run for this repo. Will ingest all available 14-day history."
            fi

            # 3. Process the 'clones' array
            # Logic: Select items where timestamp > PREV_LAST_TS. Sum them.
            
            # Save API body to temp file for jq processing
            echo "$HTTP_BODY" > temp_api.json
            
            # Calculate new additions using jq
            # If PREV_LAST_TS is empty, we select all (first run).
            # If not empty, we select only newer entries.
            jq -n \
              --slurpfile api temp_api.json \
              --arg last_ts "$PREV_LAST_TS" \
              '
              $api[0].clones 
              | if ($last_ts == "" or $last_ts == "null") then 
                  # First run: take everything
                  .
                else 
                  # Subsequent runs: only newer entries
                  map(select(.timestamp > $last_ts))
                end
              | {
                  new_total: (map(.count) | add // 0),
                  new_unique: (map(.uniques) | add // 0),
                  last_ts: (
                    if length > 0 then
                      (map(.timestamp) | max)
                    else
                      $last_ts
                    end
                  )
                }
              ' > temp_delta.json
            
            NEW_TOTAL=$(jq '.new_total' temp_delta.json)
            NEW_UNIQUE=$(jq '.new_unique' temp_delta.json)
            NEW_LAST_TS=$(jq -r '.last_ts' temp_delta.json)
            
            echo "  > New data found: +$NEW_TOTAL clones / +$NEW_UNIQUE uniques"
            
            # 4. Update Cumulative
            FINAL_TOTAL=$((CUMULATIVE_TOTAL + NEW_TOTAL))
            FINAL_UNIQUE=$((CUMULATIVE_UNIQUE + NEW_UNIQUE))
            
            # 5. Save State
            # We keep the JSON structure simple and persistent
            jq -n \
              --arg repo "$REPO_FULL_NAME" \
              --arg updated "$READABLE_TIMESTAMP" \
              --arg last_ts "$NEW_LAST_TS" \
              --argjson total "$FINAL_TOTAL" \
              --argjson unique "$FINAL_UNIQUE" \
              '{
                repository: $repo,
                last_updated: $updated,
                last_tracked_timestamp: $last_ts,
                cumulative_total: $total,
                cumulative_unique: $unique
              }' > "$STATE_FILE"
              
            # Update Ecosystem Aggregates (Summing the final states of all repos)
            ECOSYSTEM_TOTAL=$((ECOSYSTEM_TOTAL + FINAL_TOTAL))
            ECOSYSTEM_UNIQUE=$((ECOSYSTEM_UNIQUE + FINAL_UNIQUE))
            
            echo "  > Updated State: $FINAL_TOTAL total / $FINAL_UNIQUE unique"
            echo ""
            
          done < extracted_repos.txt
          
          # Clean up temp files
          rm -f temp_api.json temp_delta.json

          # 6. Save Aggregate File
          # This overwrites the aggregate with the fresh sum of all repo states
          jq -n \
            --arg ts "$READABLE_TIMESTAMP" \
            --argjson total "$ECOSYSTEM_TOTAL" \
            --argjson unique "$ECOSYSTEM_UNIQUE" \
            '{
              timestamp: $ts,
              ecosystem_totals: {
                total_clones: $total,
                unique_clones: $unique
              }
            }' > "analytics/aggregate_clones.json"
            
          echo "====================================="
          echo "ECOSYSTEM SUMMARY"
          echo "====================================="
          echo "Total Clones: $ECOSYSTEM_TOTAL"
          echo "Total Unique Cloners: $ECOSYSTEM_UNIQUE"
          echo "====================================="

      - name: Commit and push data
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add analytics/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            COMMIT_TIME=$(TZ='Etc/GMT+3' date '+%Y-%m-%d %H:%M:%S GMT-3')
            git commit -m "Update clone statistics - $COMMIT_TIME"
            git push
          fi
